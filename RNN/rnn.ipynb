{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretrained Glove embeddings path\n",
    "glove_path = '/Users/srikanth_m07/Documents/ml_dataset/nlp/wordVectors/glove.6B/glove.6B.50d.txt'\n",
    "glove_vocab = []\n",
    "glove_embd=[]\n",
    "embedding_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GLOVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(glove_path,'r',encoding='UTF-8')\n",
    "for line in file.readlines():\n",
    "    row = line.strip().split(' ')\n",
    "    vocab_word = row[0]\n",
    "    glove_vocab.append(vocab_word)\n",
    "    embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "    embedding_dict[vocab_word]=embed_vector\n",
    "file.close()\n",
    "  \n",
    "print('Loaded GLOVE')\n",
    " \n",
    "glove_vocab_size = len(glove_vocab)\n",
    "embedding_dim = len(embed_vector)\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a block of text to use as our training data.  We load the data into a numpy array for easy indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fable_text = \"\"\"\n",
    "long ago , the mice had a general council to consider what measures\n",
    "they could take to outwit their common enemy , the cat . some said\n",
    "this , and some said that but at last a young mouse got up and said\n",
    "he had a proposal to make , which he thought would meet the case . \n",
    "you will all agree , said he , that our chief danger consists in the\n",
    "sly and treacherous manner in which the enemy approaches us . now , \n",
    "if we could receive some signal of her approach , we could easily\n",
    "escape from her . i venture , therefore , to propose that a small\n",
    "bell be procured , and attached by a ribbon round the neck of the cat\n",
    ". by this means we should always know when she was about , and could\n",
    "easily retire while she was in the neighbourhood . this proposal met\n",
    "with general applause , until an old mouse got up and said that is\n",
    "all very well , but who is to bell the cat ? the mice looked at one\n",
    "another and nobody spoke . then the old mouse said it is easy to\n",
    "propose impossible remedies .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fable_text = fable_text.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long ago , the mice had a general council to consider what measuresthey could take to outwit their common enemy , the cat . some saidthis , and some said that but at last a young mouse got up and saidhe had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in thesly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easilyescape from her . i venture , therefore , to propose that a smallbell be procured , and attached by a ribbon round the neck of the cat. by this means we should always know when she was about , and couldeasily retire while she was in the neighbourhood . this proposal metwith general applause , until an old mouse got up and said that isall very well , but who is to bell the cat ? the mice looked at oneanother and nobody spoke . then the old mouse said it is easy topropose impossible remedies .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fable_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function puts all the words in a single column vector within a numpy array\n",
    "def read_data(raw_text):\n",
    "    content = raw_text\n",
    "    content = content.split() #splits the text by spaces (default split character)\n",
    "    content = np.array(content) #to array\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    " \n",
    "training_data = read_data(fable_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192,),\n",
       " array(['long', 'ago', ',', 'the', 'mice', 'had', 'a', 'general', 'council',\n",
       "        'to', 'consider', 'what', 'measuresthey', 'could', 'take', 'to',\n",
       "        'outwit', 'their', 'common', 'enemy', ',', 'the', 'cat', '.',\n",
       "        'some', 'saidthis', ',', 'and', 'some', 'said', 'that', 'but', 'at',\n",
       "        'last', 'a', 'young', 'mouse', 'got', 'up', 'and', 'saidhe', 'had',\n",
       "        'a', 'proposal', 'to', 'make', ',', 'which', 'he', 'thought',\n",
       "        'would', 'meet', 'the', 'case', '.', 'you', 'will', 'all', 'agree',\n",
       "        ',', 'said', 'he', ',', 'that', 'our', 'chief', 'danger',\n",
       "        'consists', 'in', 'thesly', 'and', 'treacherous', 'manner', 'in',\n",
       "        'which', 'the', 'enemy', 'approaches', 'us', '.', 'now', ',', 'if',\n",
       "        'we', 'could', 'receive', 'some', 'signal', 'of', 'her', 'approach',\n",
       "        ',', 'we', 'could', 'easilyescape', 'from', 'her', '.', 'i',\n",
       "        'venture', ',', 'therefore', ',', 'to', 'propose', 'that', 'a',\n",
       "        'smallbell', 'be', 'procured', ',', 'and', 'attached', 'by', 'a',\n",
       "        'ribbon', 'round', 'the', 'neck', 'of', 'the', 'cat.', 'by', 'this',\n",
       "        'means', 'we', 'should', 'always', 'know', 'when', 'she', 'was',\n",
       "        'about', ',', 'and', 'couldeasily', 'retire', 'while', 'she', 'was',\n",
       "        'in', 'the', 'neighbourhood', '.', 'this', 'proposal', 'metwith',\n",
       "        'general', 'applause', ',', 'until', 'an', 'old', 'mouse', 'got',\n",
       "        'up', 'and', 'said', 'that', 'isall', 'very', 'well', ',', 'but',\n",
       "        'who', 'is', 'to', 'bell', 'the', 'cat', '?', 'the', 'mice',\n",
       "        'looked', 'at', 'oneanother', 'and', 'nobody', 'spoke', '.', 'then',\n",
       "        'the', 'old', 'mouse', 'said', 'it', 'is', 'easy', 'topropose',\n",
       "        'impossible', 'remedies', '.'],\n",
       "       dtype='<U13'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape, training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build a dictionary and reverse dictionary that maps each word in the document vocabulary to a unique integer value.  These dictionaries serve a slightly different purpose than the dictionary with our word embeddings, but the two come together as we’ll see in  a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dictionary and reverse dictionary with word ids\n",
    "def build_dictionaries(words):\n",
    "    count = collections.Counter(words).most_common() #creates list of word/count pairs;\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    " \n",
    "word2id, id2word = build_dictionaries(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally create the array of word embeddings that we’ll actually be loading into Tensorflow.  What’s critical to realize at this point is that we probably have lots of pre-trained words in embedding_dict that aren’t in our training data and conversely we may have words in our training data that aren’t included in our pre-trained set.\n",
    "\n",
    "We solve this by looping through all the words in dictionary.  If the word is one that we already have an embedding for, then we add the embedding to a new object embeddings_tmp.  If the word is not one that we already have an embedding for, then we assign a vector of small random values.  (Don’t worry, when we train our model, we’ll allow Tensorflow to update these randomly assigned values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create embedding array\n",
    " \n",
    "doc_vocab_size = len(word2id)\n",
    "dict_as_list = sorted(word2id.items(), key = lambda x : x[1])\n",
    " \n",
    "embeddings_matrix=[]\n",
    " \n",
    "for i in range(doc_vocab_size):\n",
    "    item = dict_as_list[i][0]\n",
    "    if item in glove_vocab:\n",
    "        embeddings_matrix.append(embedding_dict[item])\n",
    "    else:\n",
    "        rand_num = np.random.uniform(low=-0.2, high=0.2,size=embedding_dim)\n",
    "        embeddings_matrix.append(rand_num)\n",
    "\n",
    "#final embedding array corresponds to dictionary of words in the document\n",
    "embedding = np.asarray(embeddings_matrix)\n",
    " \n",
    "# create tree so that we can later search for closest vector to prediction\n",
    "tree = spatial.KDTree(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up or RNN model, so this is a perfect time to start talking about tensor shapes.\n",
    "\n",
    "The most critical step in the code below is where we feed our object x_unstack into the RNN.  So if we can understand what tensor shape we need at that moment and work our way backward, we should have a good understanding of how to shape our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let’s look at the shapes of the objects we’ll be using in our code below:\n",
    "\n",
    "x: (?,3)<br>\n",
    "embedding_placeholder: (112, 300)<br>\n",
    "embedded_chars:  (?, 3, 300)<br>\n",
    "x_unstack: three separate sensors of shape (?,300)<br>\n",
    "outputs:  (?, 512)<br>\n",
    "\n",
    "Our input x is a matrix with an undetermined number rows and is three columns wide.\n",
    "\n",
    "The other input embedding_placeholder has one row per word in our document vocabulary and is 300 columns wide (to match the dimension of our word embeddings).  We initialize matrix W with the values from embedding_placeholder.\n",
    "\n",
    "Then we use the function tf.nn.embedding_lookup() to look up each of our inputs from x in matrix W resulting in the three-dimensional tensor embedded_chars that has shape (?, 3, 300).  We then unstack unto individual matrices of dimension (?,300) to feed into our RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate = 0.001\n",
    "n_input = 3 # this is the number of words that are read at a time\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# create input placeholders\n",
    "x = tf.placeholder(tf.int32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, embedding_dim])\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])\n",
    "\n",
    "#RNN output node weights and biases\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, embedding_dim])) }\n",
    "biases = { 'out': tf.Variable(tf.random_normal([embedding_dim])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[doc_vocab_size, embedding_dim]), trainable=True, name=\"W\")\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embedded_chars = tf.nn.embedding_lookup(W,x)\n",
    "\n",
    "# reshape input data\n",
    "x_unstack = tf.unstack(embedded_chars, n_input, 1)\n",
    " \n",
    "# create RNN cells\n",
    "rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(n_hidden),tf.contrib.rnn.BasicLSTMCell(n_hidden)])\n",
    "outputs, states = tf.contrib.rnn.static_rnn(rnn_cell, x_unstack, dtype=tf.float32)\n",
    " \n",
    "# capture only the last output\n",
    "pred = tf.matmul(outputs[-1], weights['out']) + biases['out'] \n",
    " \n",
    "# Create loss function and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.l2_loss(pred-y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "init=tf.global_variables_initializer()\n",
    " \n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    " \n",
    "step=0\n",
    "offset = random.randint(0,n_input+1) #random integer between 0 and 3\n",
    "end_offset = n_input+1 # in our case tihs is 4\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "training_iters = 10000\n",
    "display_step = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'a', 'ribbon'] - [round] vs [['but', '.', 'this']]\n",
      "Average Loss= 0.040058\n",
      "['last', 'a', 'young'] - [mouse] vs [['mouse', 'saidhe', 'metwith']]\n",
      "Average Loss= 0.006195\n",
      "[',', 'until', 'an'] - [old] vs [['old', 'who', 'young']]\n",
      "Average Loss= 0.002828\n",
      "['approaches', 'us', '.'] - [now] vs [['now', 'but', '.']]\n",
      "Average Loss= 0.002552\n",
      "['topropose', 'impossible', 'remedies'] - [.] vs [['.', ',', 'and']]\n",
      "Average Loss= 0.000663\n",
      "['procured', ',', 'and'] - [attached] vs [['attached', 'saidhe', 'oneanother']]\n",
      "Average Loss= 0.002070\n",
      "['to', 'make', ','] - [which] vs [['.', 'but', 'it']]\n",
      "Average Loss= 0.003199\n",
      "['said', 'that', 'isall'] - [very] vs [['very', 'but', 'this']]\n",
      "Average Loss= 0.001956\n",
      "['which', 'the', 'enemy'] - [approaches] vs [['approaches', 'approach', 'saidthis']]\n",
      "Average Loss= 0.001970\n",
      "['said', 'it', 'is'] - [easy] vs [['easy', 'make', 'always']]\n",
      "Average Loss= 0.000688\n",
      "[',', 'and', 'attached'] - [by] vs [['by', 'while', '.']]\n",
      "Average Loss= 0.002442\n",
      "['but', 'at', 'last'] - [a] vs [['a', 'an', 'the']]\n",
      "Average Loss= 0.000955\n",
      "['general', 'applause', ','] - [until] vs [['until', 'then', '.']]\n",
      "Average Loss= 0.002731\n",
      "['manner', 'in', 'which'] - [the] vs [['the', 'which', 'in']]\n",
      "Average Loss= 0.000377\n",
      "['mice', 'had', 'a'] - [general] vs [['general', 'chief', ',']]\n",
      "Average Loss= 0.001081\n",
      "['procured', ',', 'and'] - [attached] vs [['attached', 'well', 'and']]\n",
      "Average Loss= 0.001596\n",
      "['mouse', 'got', 'up'] - [and] vs [['and', 'well', ',']]\n",
      "Average Loss= 0.000608\n",
      "['was', 'in', 'the'] - [neighbourhood] vs [['neighbourhood', 'saidthis', 'consists']]\n",
      "Average Loss= 0.000611\n",
      "['he', ',', 'that'] - [our] vs [['our', 'their', 'means']]\n",
      "Average Loss= 0.000722\n",
      "['.', 'then', 'the'] - [old] vs [['old', 'a', ',']]\n",
      "Average Loss= 0.000989\n",
      "Finished Optimization\n"
     ]
    }
   ],
   "source": [
    "while step < training_iters:\n",
    "    \n",
    "    if offset > (len(training_data) - end_offset):\n",
    "        offset = random.randint(0, n_input+1)\n",
    "  # get the integer representations for the input words\n",
    "    x_integers = [[word2id[str(training_data[i])]] for i in range(offset, offset+n_input)]\n",
    "    x_integers = np.reshape(np.array(x_integers), [-1, n_input])\n",
    "  \n",
    " # create embedding for target vector \n",
    "  \n",
    "    y_position = offset+n_input\n",
    "    y_integer = word2id[training_data[y_position]]\n",
    "    y_embedding = embedding[y_integer,:]\n",
    "    y_embedding = np.reshape(y_embedding,[1,-1])\n",
    "\n",
    "  \n",
    "    _,loss, pred_ = sess.run([optimizer, cost,pred], feed_dict = {x: x_integers, y: y_embedding})\n",
    "    loss_total += loss\n",
    " \n",
    " # display output to show progress\n",
    "  \n",
    "    if (step+1) % display_step ==0:\n",
    "        words_in = [str(training_data[i]) for i in range(offset, offset+n_input)] \n",
    "        target_word = str(training_data[y_position])\n",
    "        \n",
    "        nearest_dist,nearest_idx = tree.query(pred_[0],3)\n",
    "        nearest_words = [id2word[idx] for idx in nearest_idx]\n",
    "  \n",
    "        print(\"%s - [%s] vs [%s]\" % (words_in, target_word, nearest_words))\n",
    "        print(\"Average Loss= \" + \"{:.6f}\".format(loss_total/display_step))\n",
    "    \n",
    "    loss_total=0\n",
    "    step +=1\n",
    "    offset += (n_input+1) \n",
    "\n",
    "print (\"Finished Optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_position = offset+n_input\n",
    "y_integer = word2id[training_data[y_position]]\n",
    "y_embedding = embedding[y_integer,:]\n",
    "y_embedding = np.reshape(y_embedding,[1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
